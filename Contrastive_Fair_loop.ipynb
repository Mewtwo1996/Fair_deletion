{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/graph/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.nn import GCNConv\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "from utils import (\n",
    "    get_link_labels,\n",
    "    prediction_fairness,\n",
    ")\n",
    "\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "import wandb\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_undirected, to_networkx, k_hop_subgraph, is_undirected\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import GraphSAINTRandomWalkSampler\n",
    "from torch_geometric.seed import seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from model.gcn import GCN\n",
    "from model.deletegcn import GCNDelete\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from torch_sparse import SparseTensor\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    make_scorer,\n",
    "    balanced_accuracy_score,\n",
    ")\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import model_selection, pipeline, metrics\n",
    "\n",
    "# Metrics\n",
    "from fairlearn.metrics import (\n",
    "    demographic_parity_difference,\n",
    "    equalized_odds_difference,\n",
    ")\n",
    "from itertools import combinations_with_replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric import seed_everything\n",
    "\n",
    "seed_everything(1888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from training_args import parse_args\n",
    "args=parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from model.gcn import GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = \"citeseer\" #\"cora\" \"pubmed\"\n",
    "path = osp.join(osp.dirname(osp.realpath('__file__')), \"..\", \"data\", dataset)\n",
    "dataset = Planetoid(path, dataset, transform=T.NormalizeFeatures())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/FairDrop'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/FairDrop/../data/citeseer'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_seeds = [0,1,2,3,4,5]\n",
    "acc_auc = []\n",
    "fairness = []\n",
    "acc_auc_ori = []\n",
    "fairness_ori = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[3327, 3703], edge_index=[2, 9104], y=[3327], train_mask=[3327], val_mask=[3327], test_mask=[3327])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args.in_dim=data.num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/graph/lib/python3.8/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "protected_attribute = data.y\n",
    "Y = torch.LongTensor(protected_attribute).to(device)\n",
    "data.train_mask = data.val_mask = data.test_mask = data.y = None\n",
    "data = train_test_split_edges(data, val_ratio=0.1, test_ratio=0.2)\n",
    "data = data.to(device)\n",
    "num_classes = len(np.unique(protected_attribute))\n",
    "N = data.num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3327"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191.22"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6374/100*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    1,    1,  ..., 3324, 3325, 3326],\n",
       "        [ 628,  158, 2919,  ...,  268, 1643,   33]], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_pos_edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Y = torch.LongTensor(protected_attribute).to(device)\n",
    "Y_diff = (\n",
    "    Y[data.train_pos_edge_index[0, :]] != Y[data.train_pos_edge_index[1, :]]\n",
    ").to(device)\n",
    "\n",
    "Y_same = (\n",
    "    Y[data.train_pos_edge_index[0, :]] == Y[data.train_pos_edge_index[1, :]]\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,    1,    2,  ..., 6370, 6371, 6373], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(Y_diff)\n",
    "torch.sum(Y_same)\n",
    "diff=Y_diff.nonzero().squeeze()\n",
    "same=Y_same.nonzero().squeeze()\n",
    "same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "edge_to_delete=100\n",
    "ratio=3\n",
    "diff_size=int(edge_to_delete*(ratio)/(ratio+1))\n",
    "same_size=int(edge_to_delete*(1)/(ratio+1))\n",
    "idx_diff = torch.randperm(diff.shape[0])[:diff_size]\n",
    "df_diff_idx = diff[idx_diff]\n",
    "idx_same = torch.randperm(same.shape[0])[:same_size]\n",
    "df_same_idx = same[idx_same]\n",
    "df_global_idx=torch.cat((df_diff_idx,df_same_idx),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dr_mask = torch.ones(data.train_pos_edge_index.shape[1], dtype=torch.bool)\n",
    "dr_mask[df_global_idx] = False\n",
    "dr_mask=dr_mask.to(device)\n",
    "\n",
    "df_mask = torch.zeros(data.train_pos_edge_index.shape[1], dtype=torch.bool)\n",
    "df_mask[df_global_idx] = True\n",
    "df_mask=df_mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_diff_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 0, 2, 3, 1, 3, 3, 3, 2, 1, 2, 3, 2, 1, 0, 0, 3, 0, 1, 2, 4, 5, 0,\n",
       "        2, 0, 3, 2, 0, 0, 1, 3, 5, 1, 3, 0, 3, 3, 2, 1, 1, 3, 1, 2, 3, 3, 3, 0,\n",
       "        0, 0, 0, 0, 3, 1, 2, 2, 4, 1, 1, 3, 5, 2, 2, 2, 1, 0, 4, 4, 0, 5, 1, 0,\n",
       "        2, 5, 0], device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[data.train_pos_edge_index[0,df_diff_idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 3, 0, 2, 4, 4, 2, 1, 4, 2, 1, 5, 1, 4, 4, 1, 4, 3, 2, 1, 0, 4, 4,\n",
       "        3, 1, 0, 1, 1, 4, 0, 1, 4, 5, 1, 1, 0, 2, 1, 0, 5, 4, 4, 1, 2, 4, 1, 4,\n",
       "        2, 1, 1, 1, 0, 3, 1, 1, 5, 0, 5, 1, 4, 4, 1, 5, 3, 3, 5, 0, 5, 2, 0, 2,\n",
       "        1, 0, 3], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[data.train_pos_edge_index[1,df_diff_idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unique(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3327"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N  #num of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,    1,    2,  ..., 3324, 3325, 3326])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_all=torch.arange(N)\n",
    "node_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_0=node_all[Y[node_all]==0]\n",
    "N_1=node_all[Y[node_all]==1]\n",
    "N_2=node_all[Y[node_all]==2]\n",
    "N_3=node_all[Y[node_all]==3]\n",
    "N_4=node_all[Y[node_all]==4]\n",
    "N_5=node_all[Y[node_all]==5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_node=[]\n",
    "pos_size=[]\n",
    "neg_node=[]\n",
    "neg_size=8\n",
    "for n in node_all.tolist():\n",
    "    l_hop_node, l_hop_edge, l_hop_index, l_hop_mask = k_hop_subgraph(\n",
    "                n, \n",
    "                1, \n",
    "                data.train_pos_edge_index,\n",
    "                num_nodes=data.num_nodes)\n",
    "    pos_node.append(l_hop_node[1:])\n",
    "    if(Y[n]==0):\n",
    "        neg_idx=torch.randperm(len(N_0))[0:neg_size]\n",
    "        neg_sample=N_0[neg_idx]\n",
    "    elif(Y[n]==1):\n",
    "        neg_idx=torch.randperm(len(N_1))[0:neg_size]\n",
    "        neg_sample=N_1[neg_idx]\n",
    "    elif(Y[n]==2):\n",
    "        neg_idx=torch.randperm(len(N_2))[0:neg_size]\n",
    "        neg_sample=N_2[neg_idx]\n",
    "    elif(Y[n]==3):\n",
    "        neg_idx=torch.randperm(len(N_3))[0:neg_size]\n",
    "        neg_sample=N_3[neg_idx]\n",
    "    elif(Y[n]==4):\n",
    "        neg_idx=torch.randperm(len(N_4))[0:neg_size]\n",
    "        neg_sample=N_4[neg_idx]\n",
    "    else:\n",
    "        neg_idx=torch.randperm(len(N_5))[0:neg_size]\n",
    "        neg_sample=N_5[neg_idx]  \n",
    "    neg_node.append(neg_sample)\n",
    "    pos_size.append(len(l_hop_node)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Aug \n",
    "\n",
    "def init_weights(m): \n",
    "    if isinstance(m, nn.Linear): \n",
    "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu') \n",
    "        nn.init.constant_(m.bias, 0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "\n",
    "class MLPA(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_feats, dim_h, dim_z):\n",
    "        super(MLPA, self).__init__()\n",
    "        \n",
    "        self.gcn_mean = torch.nn.Sequential(\n",
    "                torch.nn.Linear(in_feats, dim_h),\n",
    "                torch.nn.BatchNorm1d(dim_h),\n",
    "                torch.nn.LeakyReLU(),\n",
    "                torch.nn.Linear(dim_h, dim_z)\n",
    "                )\n",
    "\n",
    "    def forward(self, hidden):\n",
    "        # GCN encoder\n",
    "        self.gcn_mean.apply(init_weights)\n",
    "        Z = self.gcn_mean(hidden)\n",
    "        # inner product decoder\n",
    "        adj_logits = Z @ Z.T\n",
    "        return adj_logits\n",
    "\n",
    "class PGNNMask(torch.nn.Module):\n",
    "    def __init__(self, features, n_hidden=64, temperature=1) -> None:\n",
    "        super(PGNNMask,self).__init__()\n",
    "        #self.g_encoder = GCN_Body(in_feats = features.shape[1], n_hidden = n_hidden, out_feats = n_hidden, dropout = 0.1, nlayer = 1)\n",
    "        self.Aaug = MLPA(in_feats = n_hidden, dim_h = n_hidden, dim_z =features.shape[1])\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def _sample_graph(self, sampling_weights, temperature=1.0, bias=0.0, training=True):\n",
    "        if training:\n",
    "            bias = bias + 0.0001  # If bias is 0, we run into problems\n",
    "            eps = (bias - (1-bias)) * torch.rand(sampling_weights.size()) + (1-bias)\n",
    "            gate_inputs = torch.log(eps) - torch.log(1 - eps)\n",
    "            gate_inputs=gate_inputs.cuda()\n",
    "            gate_inputs = (gate_inputs + sampling_weights) / temperature\n",
    "            graph =  torch.sigmoid(gate_inputs)\n",
    "        else:\n",
    "            graph = torch.sigmoid(sampling_weights)\n",
    "        return graph\n",
    "    \n",
    "    def normalize_adj(self,adj):\n",
    "        adj.fill_diagonal_(1)\n",
    "        # normalize adj with A = D^{-1/2} @ A @ D^{-1/2}\n",
    "        D_norm = torch.diag(torch.pow(adj.sum(1), -0.5)).cuda()\n",
    "        adj = D_norm @ adj @ D_norm\n",
    "        return adj\n",
    "\n",
    "    def forward(self, h, alpha = 0.5, adj_orig = None):\n",
    "        #h = self.g_encoder(adj, x)\n",
    "\n",
    "        # Edge perturbation\n",
    "        adj_logits = self.Aaug(h)\n",
    "        ## sample a new adj\n",
    "        edge_probs = torch.sigmoid(adj_logits)\n",
    "\n",
    "        if (adj_orig is not None) :\n",
    "            edge_probs = alpha*edge_probs + (1-alpha)*adj_orig\n",
    "\n",
    "        # sampling \n",
    "        adj_sampled =self._sample_graph(adj_logits)\n",
    "        # making adj_sampled symmetric\n",
    "        adj_sampled = adj_sampled.triu(1)\n",
    "        adj_sampled = adj_sampled + adj_sampled.T\n",
    "        adj_sampled = self.normalize_adj(adj_sampled)\n",
    "\n",
    "\n",
    "        return adj_sampled, adj_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9158400961827473"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(pos_size)/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6374])\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "full_pos_edge_index1=data.train_pos_edge_index\n",
    "print(full_pos_edge_index1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    1,    1,  ..., 3324, 3325, 3326],\n",
       "        [ 628,  158, 2919,  ...,  268, 1643,   33]], device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_pos_edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_undirected(data.train_pos_edge_index) ## data.train_pos_edge_index already undirected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_edges_set=set(list((data.train_pos_edge_index[0,i].tolist(), data.train_pos_edge_index[1,i].tolist()) for i in range(data.train_pos_edge_index.shape[1]) ))\n",
    "full_node_list=data.train_pos_edge_index[0,:].tolist()\n",
    "delete_node_list=list(set(data.train_pos_edge_index[:, df_mask][0,:].tolist()+data.train_pos_edge_index[:, df_mask][1,:].tolist()))\n",
    "#forget_edges_set=list(product(data.train_pos_edge_index[0,:].tolist(), data.train_pos_edge_index[1,:].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#full_edges_set\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    1,    1,  ..., 3324, 3325, 3326],\n",
       "        [ 628,  158, 2919,  ...,  268, 1643,   33]], device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_pos_edge_index=copy.deepcopy(full_pos_edge_index1)\n",
    "full_pos_edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_pos_edge_index[:, df_mask].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6374])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_pos_edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6274])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_pos_edge_index1[:,dr_mask].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "re_edge_num=full_pos_edge_index1[:,dr_mask].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##potential edges\n",
    "full_pos_edge_index=copy.deepcopy(full_pos_edge_index1)[:,dr_mask]\n",
    "existing_edges=full_edges_set\n",
    "k=1\n",
    "#for target in range(edge_to_delete):\n",
    "for target in df_diff_idx:\n",
    "    _, l_hop_edge, _, l_hop_mask = k_hop_subgraph(\n",
    "            [data.train_pos_edge_index[0,:][target]], \n",
    "            k, \n",
    "            data.train_pos_edge_index,\n",
    "            num_nodes=data.num_nodes)\n",
    "\n",
    "    ldset1=l_hop_edge.flatten().unique()\n",
    "    ldset1=ldset1[ldset1!=data.train_pos_edge_index[0,:][target].item()]\n",
    "\n",
    "    _, r_hop_edge, _, r_hop_mask = k_hop_subgraph(\n",
    "            [data.train_pos_edge_index[1,:][target]], \n",
    "            k, \n",
    "            data.train_pos_edge_index,\n",
    "            num_nodes=data.num_nodes)\n",
    "\n",
    "    rdset1=r_hop_edge.flatten().unique()\n",
    "    rdset1=rdset1[rdset1!=data.train_pos_edge_index[1,:][target].item()]\n",
    "\n",
    "    combine = list(product(ldset1.tolist(), rdset1.tolist()))\n",
    "    ind=[True if Y[a[0]]!=Y[a[1]] else False for a in combine ]\n",
    "    sele_pair=[combine[i] for i in range(len(ind)) if ind[i]==True]\n",
    "    #print(\"sele_pair\",len(sele_pair))\n",
    "    ind1=[False if (a[0],a[1]) in existing_edges else True for a in sele_pair]\n",
    "    sele_pair1=[sele_pair[i] for i in range(len(ind1)) if ind1[i]==True]\n",
    "    #print(\"sele_pair1\",len(sele_pair1))\n",
    "    existing_edges=set(list(sele_pair1)+list(existing_edges)+list((a[1],a[0])for a in sele_pair1))\n",
    "    #print(\"existing\",len(existing_edges))\n",
    "    n=len(sele_pair1)\n",
    "    #add=n\n",
    "    #add_pair_ind=torch.randperm(len(sele_pair))[:add]\n",
    "    add_pair_ind=np.arange(len(sele_pair1))\n",
    "    add_pair=[sele_pair1[i] for i in add_pair_ind]\n",
    "    add_matrix_0=[add_pair[i][0] for i in range(len(add_pair))] \n",
    "    add_matrix_1=[add_pair[i][1] for i in range(len(add_pair))] \n",
    "    add_matrix1=torch.tensor([add_matrix_0,add_matrix_1]).to(device)\n",
    "    add_matrix2=torch.tensor([add_matrix_1,add_matrix_0]).to(device)\n",
    "    full_pos_edge_index=torch.cat((full_pos_edge_index,add_matrix1),1)\n",
    "    full_pos_edge_index=torch.cat((full_pos_edge_index,add_matrix2),1)\n",
    "full_pos_edge_index=full_pos_edge_index.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6680])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_pos_edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.aug_pos_edge_index=full_pos_edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (conv1): GCNConv(3703, 128)\n",
       "  (conv2): GCNConv(128, 64)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ckpt = torch.load(os.path.join(args.checkpoint_dir, 'model_final.pt'), map_location=device)\n",
    "model_ori=GCN(args)\n",
    "model_ori.load_state_dict(model_ckpt['model_state'], strict=False)\n",
    "model_ori=model_ori.to(device)\n",
    "model_ori.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from model.Auggcn import AugGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AugGCN(\n",
       "  (conv1): GCNConv(3703, 128)\n",
       "  (conv2): GCNConv(128, 64)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_aug = AugGCN(args)\n",
    "model_ckpt = torch.load(os.path.join(args.checkpoint_dir, 'model_final.pt'), map_location=device)\n",
    "model_aug.load_state_dict(model_ckpt['model_state'], strict=False)\n",
    "model_aug = model_aug.to(device)\n",
    "model_aug.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "z_ori= model_ori(data.x, data.train_pos_edge_index,return_all_emb=False)\n",
    "z_ori.detach()\n",
    "z_ori.shape\n",
    "aug = PGNNMask(data.x.cuda(), n_hidden=64, temperature=6).to(device)\n",
    "aug=aug.to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(reduction='none')\n",
    "optimizer_aug = torch.optim.Adam(aug.parameters(), lr = 0.001)\n",
    "\n",
    "adj_sampled_aug,adj_logits_aug = aug(z_ori.cuda())\n",
    "\n",
    "weight_aug1=adj_sampled_aug[full_pos_edge_index[0],full_pos_edge_index[1]]\n",
    "#weight_aug[:6374]=1.0\n",
    "#weight_aug[:6374]=weight_aug[:6374].detach()\n",
    "weight_ori=torch.zeros(full_pos_edge_index.shape[1]).to(device)\n",
    "weight_ori[:re_edge_num]=1.0\n",
    "weight_ori=weight_ori.detach()\n",
    "m=1-weight_ori\n",
    "m=m.detach()\n",
    "weight_aug=weight_ori+m*weight_aug1*800\n",
    "z_aug = model_aug(data.x, data.aug_pos_edge_index,weight_aug,return_all_emb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###aug \n",
    "#torch.nn.utils.clip_grad_norm_(aug.parameters(),max_norm=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "node_record=[]\n",
    "k=2\n",
    "for target in range(edge_to_delete):\n",
    "    l_hop_node, l_hop_edge, l_hop_index, l_hop_mask = k_hop_subgraph(\n",
    "            [data.train_pos_edge_index[:, df_mask][0,:][target]], \n",
    "            k, \n",
    "            data.train_pos_edge_index,\n",
    "            num_nodes=data.num_nodes)   \n",
    "\n",
    "    r_hop_node, r_hop_edge, _, r_hop_mask = k_hop_subgraph(\n",
    "            [data.train_pos_edge_index[:, df_mask][1,:][target]], \n",
    "            k, \n",
    "            data.train_pos_edge_index,\n",
    "            num_nodes=data.num_nodes)\n",
    "    node_record+=l_hop_node.tolist()\n",
    "    node_record+=r_hop_node.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "963\n"
     ]
    }
   ],
   "source": [
    "node=set(node_record)\n",
    "print(len(node))\n",
    "target_node=torch.tensor(list(node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_idx=torch.arange(len(weight_aug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "loss tensor(0.8565, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "weight_max tensor(0.2410, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "dif tensor(97.6555, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "l tensor(0, device='cuda:0')\n",
      "epoch 100\n",
      "loss tensor(0.8431, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "weight_max tensor(0.2413, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "dif tensor(97.6572, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "l tensor(0, device='cuda:0')\n",
      "epoch 200\n",
      "loss tensor(0.8468, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "weight_max tensor(0.2430, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "dif tensor(97.8326, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "l tensor(0, device='cuda:0')\n",
      "epoch 300\n",
      "loss tensor(0.8462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "weight_max tensor(0.2443, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "dif tensor(97.8286, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "l tensor(0, device='cuda:0')\n",
      "epoch 400\n",
      "loss tensor(0.8584, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "weight_max tensor(0.2453, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "dif tensor(98.2611, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "l tensor(0, device='cuda:0')\n",
      "epoch 500\n",
      "loss tensor(0.8502, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "weight_max tensor(0.2659, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "dif tensor(100.0221, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "l tensor(0, device='cuda:0')\n",
      "epoch 600\n",
      "loss tensor(0.8446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "weight_max tensor(0.3323, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "dif tensor(104.8071, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "l tensor(0, device='cuda:0')\n",
      "epoch 700\n",
      "loss tensor(0.8400, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "weight_max tensor(0.5302, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "dif tensor(132.0788, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "l tensor(0, device='cuda:0')\n",
      "Add tensor(0.8519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "l tensor(2, device='cuda:0')\n",
      "weight_max tensor(1.0748, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "add tensor([[1749, 1910],\n",
      "        [1910, 1749]], device='cuda:0')\n",
      "epoch 800\n",
      "loss tensor(0.8597, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "weight_max tensor(0.5632, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "dif tensor(154.7145, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "l tensor(0, device='cuda:0')\n",
      "epoch 900\n",
      "loss tensor(0.8473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "weight_max tensor(0.6513, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "dif tensor(199.2795, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "l tensor(0, device='cuda:0')\n",
      "epoch 1000\n",
      "loss tensor(0.8426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "weight_max tensor(0.6783, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "dif tensor(231.4978, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "l tensor(0, device='cuda:0')\n",
      "Add tensor(0.8509, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "l tensor(2, device='cuda:0')\n",
      "weight_max tensor(0.9024, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "add tensor([[2789, 1352],\n",
      "        [1352, 2789]], device='cuda:0')\n",
      "epoch 1100\n",
      "loss tensor(0.8498, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "weight_max tensor(0.7997, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "dif tensor(240.2124, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "l tensor(0, device='cuda:0')\n",
      "Add tensor(0.8479, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "l tensor(26, device='cuda:0')\n",
      "weight_max tensor(0.9945, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "add tensor([[ 548, 2613, 2613,   61, 2912, 2613],\n",
      "        [2613,  548,   61, 2613, 2613, 2912]], device='cuda:0')\n",
      "Add tensor(0.8512, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "l tensor(20, device='cuda:0')\n",
      "weight_max tensor(0.9614, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "add tensor([[ 492, 2681, 2681,  887, 2585, 2681],\n",
      "        [2681,  492,  887, 2681, 2681, 2585]], device='cuda:0')\n",
      "epoch 1200\n",
      "loss tensor(0.8471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "weight_max tensor(0.7304, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "dif tensor(222.2692, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "l tensor(0, device='cuda:0')\n",
      "Add tensor(0.8449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "l tensor(16, device='cuda:0')\n",
      "weight_max tensor(0.9693, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "add tensor([[2128, 2681, 2681,   27, 2257, 2681],\n",
      "        [2681, 2128,   27, 2681, 2681, 2257]], device='cuda:0')\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##real\n",
    "batch_size=256\n",
    "weight=0.0000001\n",
    "#weight=0\n",
    "l_num=0\n",
    "max_edge=20\n",
    "max_per_iter=3\n",
    "add_edge=0\n",
    "edge_add=[]\n",
    "for epoch in range(5000):\n",
    "    batch_idx=target_node[torch.randperm(len(target_node))[0:batch_size]]\n",
    "    #aug.train()\n",
    "    for i in range(1):\n",
    "        weight_aug1=adj_sampled_aug[full_pos_edge_index[0],full_pos_edge_index[1]]\n",
    "        weight_aug=weight_ori+m*weight_aug1*800\n",
    "        z_aug = model_aug(data.x, data.aug_pos_edge_index,weight_aug,return_all_emb=False)\n",
    "        #print(z_aug.requires_grad)\n",
    "        pos_sim_padded = []\n",
    "        neg_sim_batch=[]\n",
    "        mask_padded = []\n",
    "        max_num_pos=5\n",
    "        #batch_idx=torch.randperm(N)[0:batch_size]\n",
    "        #print(batch_idx)\n",
    "        #batch_idx=target_node[torch.randperm(len(target_node))[0:batch_size]]\n",
    "        for idx in batch_idx:\n",
    "            #print(idx)\n",
    "            z_pos_aug=z_aug[pos_node[idx],:]\n",
    "            z_neg_aug=z_aug[neg_node[idx],:]\n",
    "            pos_sim=torch.matmul(z_pos_aug,z_aug[idx].unsqueeze(1)).T\n",
    "            #print(\"pos_sim\",pos_sim.shape)\n",
    "            neg_sim=torch.matmul(z_neg_aug,z_aug[idx].unsqueeze(1)).T\n",
    "            #print(\"neg_sim\",neg_sim.shape)\n",
    "            neg_sim_batch.append(neg_sim)\n",
    "            padding_size = max_num_pos - pos_sim.size(1)\n",
    "            #print(F.pad(pos_sim, (0, padding_size), value=0.0))\n",
    "            pos_sim_padded.append(F.pad(pos_sim, (0, padding_size), value=0.0))  # Pad with 0 for missing positives\n",
    "            #print(\"append\",len(pos_sim_padded))\n",
    "            mask_padded.append(F.pad(torch.ones_like(pos_sim), (0, padding_size), value=0.0))  # Mask for real values\n",
    "            # Stack the padded similarities and masks\n",
    "        pos_sim_padded = torch.stack(pos_sim_padded,dim=0).squeeze(dim=1)  # [batch_size, max_num_positives]\n",
    "        neg_sim_batch=torch.stack(neg_sim_batch,dim=0).squeeze(dim=1)\n",
    "        mask_padded = torch.stack(mask_padded,dim=0).squeeze(dim=1)  # [batch_size, max_num_positives]\n",
    "        #z_aug = model_aug(data.x, data.aug_pos_edge_index,weight_aug,return_all_emb=False)\n",
    "        # Concatenate negative and padded positive similarities to form logits\n",
    "        logits = torch.cat([neg_sim_batch, pos_sim_padded], dim=1)  # [batch_size, num_negatives + max_num_positives]\n",
    "\n",
    "        # Create labels: 0 for negative pairs, 1 for real positive pairs (mask out padded entries)\n",
    "        labels = torch.cat([torch.zeros_like(neg_sim_batch), mask_padded], dim=1)  # [batch_size, num_negatives + max_num_positives]\n",
    "        link_probs = logits.sigmoid()\n",
    "        # Compute Binary Cross-Entropy loss without reduction to apply mask\n",
    "        loss = loss_fn(link_probs, labels)\n",
    "\n",
    "        # Mask the padded positions by multiplying the loss by the mask (for positive part)\n",
    "        mask = torch.cat([torch.ones_like(neg_sim_batch), mask_padded], dim=1)  # Mask for ignoring padded logits\n",
    "        #print(\"z_aug\",z_aug.requires_grad)\n",
    "        loss = loss * mask  # Apply mask to ignore padded positions\n",
    "        for name, param in model_aug.named_parameters():\n",
    "            param.requires_grad=False\n",
    "        l1_loss=torch.abs(weight_aug[re_edge_num:]).sum()\n",
    "        loss_total=(loss.sum()/mask.sum())+l1_loss*weight\n",
    "        \n",
    "        #optimizer_aug.zero_grad()\n",
    "        loss_total.backward(retain_graph=True)\n",
    "        #print(\"loss_total\",loss_total)\n",
    "        optimizer_aug.step()\n",
    "        optimizer_aug.zero_grad()\n",
    "        z_aug_detach=z_aug.detach()\n",
    "        \"\"\"\n",
    "        for name, param in aug.named_parameters():\n",
    "            param.requires_grad=False\n",
    "        print(\"z_aug1\",z_aug.requires_grad)\n",
    "        \"\"\"\n",
    "        adj_sampled_aug,adj_logits_aug = aug(z_aug_detach)\n",
    "        #adj_sampled_aug,adj_logits_aug = aug(z_ori.cuda())\n",
    "        #l=weight_aug[re_edge_num:]>0.9\n",
    "        l=(m*weight_aug1*800)>0.9\n",
    "        l_num=l.sum()\n",
    "    if(l_num>0):\n",
    "            print(\"Add\",loss.sum()/mask.sum())\n",
    "            print(\"l\",l_num)\n",
    "            print(\"weight_max\",(m*weight_aug1*800).max())\n",
    "            num_add=min(l_num,max_per_iter*2)\n",
    "            if(num_add%2==0):\n",
    "                v,idx=torch.topk((m*weight_aug1*800),num_add)\n",
    "                weight_ori[idx]=1\n",
    "                m=1-weight_ori\n",
    "                for i in idx:\n",
    "                    edge_add.append((data.aug_pos_edge_index[0,i].item(),data.aug_pos_edge_index[1,i].item()))\n",
    "                print(\"add\",data.aug_pos_edge_index[:,idx])\n",
    "                if(len(edge_add)>max_edge*2):\n",
    "                    print(\"finish\")\n",
    "                    break\n",
    "            \n",
    "    \n",
    "#weight_aug[:6374]=weight_aug[:6374].detach()\n",
    "    if(epoch%100==0):\n",
    "        print(\"epoch\",epoch)\n",
    "        print(\"loss\",loss.sum()/mask.sum())\n",
    "        print(\"weight_max\",(m*weight_aug1*800).max())\n",
    "        dif=(weight_aug-weight_ori).sum()\n",
    "        print(\"dif\",dif)\n",
    "        print(\"l\",l_num)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1749, 1910),\n",
       " (1910, 1749),\n",
       " (2789, 1352),\n",
       " (1352, 2789),\n",
       " (548, 2613),\n",
       " (2613, 548),\n",
       " (2613, 61),\n",
       " (61, 2613),\n",
       " (2912, 2613),\n",
       " (2613, 2912),\n",
       " (492, 2681),\n",
       " (2681, 492),\n",
       " (2681, 887),\n",
       " (887, 2681),\n",
       " (2585, 2681),\n",
       " (2681, 2585),\n",
       " (2128, 2681),\n",
       " (2681, 2128),\n",
       " (2681, 27),\n",
       " (27, 2681),\n",
       " (2257, 2681),\n",
       " (2681, 2257)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "edge_new=torch.tensor([[edge_add[i][0],edge_add[i][1]] for i in range(len(edge_add))]).T.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 22])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6374])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_pos_edge_index1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_pos_edge_index_aug=torch.cat((full_pos_edge_index1,edge_new),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.aug_pos_edge_index=full_pos_edge_index_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dr_full_mask = torch.ones(data.aug_pos_edge_index.shape[1], dtype=torch.bool)\n",
    "dr_full_mask[df_global_idx] = False\n",
    "dr_full_mask=dr_full_mask.to(device)\n",
    "\n",
    "df_full_mask = torch.zeros(data.aug_pos_edge_index.shape[1], dtype=torch.bool)\n",
    "df_full_mask[df_global_idx] = True\n",
    "df_full_mask=df_full_mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Edges in S_Df\n",
    "_, two_hop_edge, _, two_hop_mask = k_hop_subgraph(\n",
    "        data.aug_pos_edge_index[:, df_full_mask].flatten().unique(), \n",
    "        2, \n",
    "        data.aug_pos_edge_index,\n",
    "        num_nodes=data.num_nodes)\n",
    "data.sdf_mask = two_hop_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Nodes in S_Df\n",
    "_, one_hop_edge, _, one_hop_mask = k_hop_subgraph(\n",
    "    data.aug_pos_edge_index[:, df_full_mask].flatten().unique(), \n",
    "    1, \n",
    "    data.aug_pos_edge_index,\n",
    "    num_nodes=data.num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sdf_node_1hop = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "sdf_node_2hop = torch.zeros(data.num_nodes, dtype=torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sdf_node_1hop[one_hop_edge.flatten().unique()] = True\n",
    "sdf_node_2hop[two_hop_edge.flatten().unique()] = True\n",
    "assert sdf_node_1hop.sum() == len(one_hop_edge.flatten().unique())\n",
    "assert sdf_node_2hop.sum() == len(two_hop_edge.flatten().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.sdf_node_1hop_mask = sdf_node_1hop\n",
    "data.sdf_node_2hop_mask = sdf_node_2hop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_undirected(data.aug_pos_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_pos_edge_index1, [df_full_mask1, two_hop_mask1] = to_undirected(data.aug_pos_edge_index, [df_full_mask.int(), two_hop_mask.int()])\n",
    "two_hop_mask1 = two_hop_mask1.bool()\n",
    "df_full_mask1 = df_full_mask1.bool()\n",
    "dr_full_mask1 = ~df_full_mask\n",
    "\n",
    "data.aug_pos_edge_index1 =full_pos_edge_index1\n",
    "data.edge_index1 = full_pos_edge_index1\n",
    "assert is_undirected(data.aug_pos_edge_index1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.sdf_mask = two_hop_mask1\n",
    "data.df_aug_mask = df_full_mask1\n",
    "data.dr_aug_mask = dr_full_mask1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3327, 64])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_aug= model_ori(data.x, data.aug_pos_edge_index1[:, data.sdf_mask],return_all_emb=False)\n",
    "z_aug.detach()\n",
    "z_aug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask = PGNNMask(data.x.cuda(), n_hidden=64, temperature=1).to(device)\n",
    "mask=mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from model.Wdeletegcn import WGCNDelete\n",
    "model_pru = WGCNDelete(args, sdf_node_1hop, sdf_node_2hop, num_nodes=data.num_nodes, num_edge_type=args.num_edge_type)\n",
    "model_ckpt = torch.load(os.path.join(args.checkpoint_dir, 'model_final.pt'), map_location=device)\n",
    "model_pru.load_state_dict(model_ckpt['model_state'], strict=False)\n",
    "model_pru = model_pru.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters_to_optimize ['deletion1.deletion_weight', 'deletion2.deletion_weight']\n"
     ]
    }
   ],
   "source": [
    "parameters_to_optimize = [\n",
    "                {'params': [p for n, p in model_pru.named_parameters() if 'del' in n], 'weight_decay': 0.0}\n",
    "            ]\n",
    "print('parameters_to_optimize', [n for n, p in model_pru.named_parameters() if 'del' in n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer_mask = torch.optim.Adam(mask.parameters(), lr = 1e-2)\n",
    "optimizer_pru = torch.optim.Adam(parameters_to_optimize, lr=0.00001)#, weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sdf1_all_pair_mask = torch.zeros(data.num_nodes, data.num_nodes, dtype=torch.bool)\n",
    "idx = torch.combinations(torch.arange(data.num_nodes)[data.sdf_node_1hop_mask], with_replacement=True).t()\n",
    "sdf1_all_pair_mask[idx[0], idx[1]] = True\n",
    "sdf1_all_pair_mask[idx[1], idx[0]] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert sdf1_all_pair_mask.sum().cpu() == data.sdf_node_1hop_mask.sum().cpu() * data.sdf_node_1hop_mask.sum().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " ## Remove Df itself\n",
    "sdf1_all_pair_mask[data.aug_pos_edge_index1[:, data.df_aug_mask][0], data.aug_pos_edge_index1[:, data.df_aug_mask][1]] = False\n",
    "sdf1_all_pair_mask[data.aug_pos_edge_index1[:, data.df_aug_mask][1], data.aug_pos_edge_index1[:, data.df_aug_mask][0]] = False\n",
    "##sdf1_all_pair_mask contain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sdf2_all_pair_mask = torch.zeros(data.num_nodes, data.num_nodes, dtype=torch.bool)\n",
    "idx = torch.combinations(torch.arange(data.num_nodes)[data.sdf_node_2hop_mask], with_replacement=True).t()\n",
    "sdf2_all_pair_mask[idx[0], idx[1]] = True\n",
    "sdf2_all_pair_mask[idx[1], idx[0]] = True\n",
    "assert sdf2_all_pair_mask.sum().cpu() == data.sdf_node_2hop_mask.sum().cpu() * data.sdf_node_2hop_mask.sum().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " ## Remove Df itself\n",
    "sdf2_all_pair_mask[data.aug_pos_edge_index1[:, data.df_aug_mask][0], data.aug_pos_edge_index1[:, data.df_aug_mask][1]] = False\n",
    "sdf2_all_pair_mask[data.aug_pos_edge_index1[:, data.df_aug_mask][1], data.aug_pos_edge_index1[:, data.df_aug_mask][0]] = False\n",
    "##sdf1_all_pair_mask contain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " ## Lower triangular mask\n",
    "idx = torch.tril_indices(data.num_nodes, data.num_nodes, -1)\n",
    "lower_mask = torch.zeros(data.num_nodes, data.num_nodes, dtype=torch.bool)\n",
    "lower_mask[idx[0], idx[1]] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## The final mask is the intersection\n",
    "sdf1_all_pair_without_df_mask = sdf1_all_pair_mask & lower_mask\n",
    "sdf2_all_pair_without_df_mask = sdf2_all_pair_mask & lower_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[3327, 3703], val_pos_edge_index=[2, 455], test_pos_edge_index=[2, 910], train_pos_edge_index=[2, 6374], train_neg_adj_mask=[3327, 3327], val_neg_edge_index=[2, 455], test_neg_edge_index=[2, 910], aug_pos_edge_index=[2, 6396], sdf_mask=[6396], sdf_node_1hop_mask=[3327], sdf_node_2hop_mask=[3327], aug_pos_edge_index1=[2, 6396], edge_index1=[2, 6396], df_aug_mask=[6396], dr_aug_mask=[6396], sdf_node_1hop_mask_non_df_mask=[3327], sdf_node_2hop_mask_non_df_mask=[3327])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "non_df_node_mask = torch.ones(data.x.shape[0], dtype=torch.bool, device=data.x.device)\n",
    "non_df_node_mask[data.aug_pos_edge_index1[:,data.df_aug_mask].flatten().unique()] = False\n",
    "\n",
    "data.sdf_node_1hop_mask_non_df_mask = data.sdf_node_1hop_mask.to(device) & non_df_node_mask\n",
    "data.sdf_node_2hop_mask_non_df_mask = data.sdf_node_2hop_mask.to(device) & non_df_node_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fct = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "z_ori= model_ori(data.x, data.aug_pos_edge_index1[:, data.sdf_mask],return_all_emb=False)\n",
    "logits_ori=(z_ori @ z_ori.t())\n",
    "logits_ori=logits_ori.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "neg_size = data.df_aug_mask.sum()\n",
    "neg_edge = negative_sampling(\n",
    "    edge_index=data.aug_pos_edge_index1,\n",
    "    num_nodes=data.num_nodes,\n",
    "    num_neg_samples=neg_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(200, device='cuda:0')"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_val(model,weight):\n",
    "    model.eval()\n",
    "    weight.detach()\n",
    "    perfs = []\n",
    "    for prefix in [\"val\", \"test\"]:\n",
    "        pos_edge_index = data[f\"{prefix}_pos_edge_index\"]\n",
    "        neg_edge_index = data[f\"{prefix}_neg_edge_index\"]\n",
    "        with torch.no_grad():\n",
    "            z = model(data.x, data.aug_pos_edge_index1,weight)\n",
    "            link_logits = model.decode(z, pos_edge_index, neg_edge_index)\n",
    "        link_probs = link_logits.sigmoid()\n",
    "        link_labels = get_link_labels(pos_edge_index, neg_edge_index)\n",
    "        auc = roc_auc_score(link_labels.cpu(), link_probs.cpu())\n",
    "        perfs.append(auc)\n",
    "    return perfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_forget(model,weight):\n",
    "    model.eval()\n",
    "    weight.detach()\n",
    "    neg_edge_index=data.aug_pos_edge_index1[:,data.df_aug_mask]\n",
    "    pos_edge_index=data.aug_pos_edge_index1[:,same[torch.randperm(same.shape[0])[:edge_to_delete]]]\n",
    "    with torch.no_grad():\n",
    "        z = model(data.x, data.aug_pos_edge_index1,weight)\n",
    "        link_logits = model.decode(z, pos_edge_index, neg_edge_index)\n",
    "    link_probs = link_logits.sigmoid()\n",
    "    link_labels = get_link_labels(pos_edge_index, neg_edge_index)\n",
    "    auc = roc_auc_score(link_labels.cpu(), link_probs.cpu())\n",
    "    return auc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ckpt = {\n",
    "            'model_state': model_pru.state_dict(),\n",
    "            'optimizer_state': optimizer_pru.state_dict(),\n",
    "        }\n",
    "best_forget=1\n",
    "best_auc=0\n",
    "select=\"forget\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6396])"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.aug_pos_edge_index1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6396])"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sdf_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_ori.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##real\n",
    "batch_size=256\n",
    "weight=0.0000001\n",
    "#weight=0\n",
    "l_num=0\n",
    "max_edge=20\n",
    "max_per_iter=3\n",
    "add_edge=0\n",
    "edge_add=[]\n",
    "for epoch in range(5000):\n",
    "    batch_idx=target_node[torch.randperm(len(target_node))[0:batch_size]]\n",
    "    #aug.train()\n",
    "    for i in range(1):\n",
    "        weight_aug1=adj_sampled_aug[full_pos_edge_index[0],full_pos_edge_index[1]]\n",
    "        weight_aug=weight_ori+m*weight_aug1*800\n",
    "        z_aug = model_aug(data.x, data.aug_pos_edge_index,weight_aug,return_all_emb=False)\n",
    "        #print(z_aug.requires_grad)\n",
    "        pos_sim_padded = []\n",
    "        neg_sim_batch=[]\n",
    "        mask_padded = []\n",
    "        max_num_pos=5\n",
    "        #batch_idx=torch.randperm(N)[0:batch_size]\n",
    "        #print(batch_idx)\n",
    "        #batch_idx=target_node[torch.randperm(len(target_node))[0:batch_size]]\n",
    "        for idx in batch_idx:\n",
    "            #print(idx)\n",
    "            z_pos_aug=z_aug[pos_node[idx],:]\n",
    "            z_neg_aug=z_aug[neg_node[idx],:]\n",
    "            pos_sim=torch.matmul(z_pos_aug,z_aug[idx].unsqueeze(1)).T\n",
    "            #print(\"pos_sim\",pos_sim.shape)\n",
    "            neg_sim=torch.matmul(z_neg_aug,z_aug[idx].unsqueeze(1)).T\n",
    "            #print(\"neg_sim\",neg_sim.shape)\n",
    "            neg_sim_batch.append(neg_sim)\n",
    "            padding_size = max_num_pos - pos_sim.size(1)\n",
    "            #print(F.pad(pos_sim, (0, padding_size), value=0.0))\n",
    "            pos_sim_padded.append(F.pad(pos_sim, (0, padding_size), value=0.0))  # Pad with 0 for missing positives\n",
    "            #print(\"append\",len(pos_sim_padded))\n",
    "            mask_padded.append(F.pad(torch.ones_like(pos_sim), (0, padding_size), value=0.0))  # Mask for real values\n",
    "            # Stack the padded similarities and masks\n",
    "        pos_sim_padded = torch.stack(pos_sim_padded,dim=0).squeeze(dim=1)  # [batch_size, max_num_positives]\n",
    "        neg_sim_batch=torch.stack(neg_sim_batch,dim=0).squeeze(dim=1)\n",
    "        mask_padded = torch.stack(mask_padded,dim=0).squeeze(dim=1)  # [batch_size, max_num_positives]\n",
    "        #z_aug = model_aug(data.x, data.aug_pos_edge_index,weight_aug,return_all_emb=False)\n",
    "        # Concatenate negative and padded positive similarities to form logits\n",
    "        logits = torch.cat([neg_sim_batch, pos_sim_padded], dim=1)  # [batch_size, num_negatives + max_num_positives]\n",
    "\n",
    "        # Create labels: 0 for negative pairs, 1 for real positive pairs (mask out padded entries)\n",
    "        labels = torch.cat([torch.zeros_like(neg_sim_batch), mask_padded], dim=1)  # [batch_size, num_negatives + max_num_positives]\n",
    "        link_probs = logits.sigmoid()\n",
    "        # Compute Binary Cross-Entropy loss without reduction to apply mask\n",
    "        loss = loss_fn(link_probs, labels)\n",
    "\n",
    "        # Mask the padded positions by multiplying the loss by the mask (for positive part)\n",
    "        mask = torch.cat([torch.ones_like(neg_sim_batch), mask_padded], dim=1)  # Mask for ignoring padded logits\n",
    "        #print(\"z_aug\",z_aug.requires_grad)\n",
    "        loss = loss * mask  # Apply mask to ignore padded positions\n",
    "        for name, param in model_aug.named_parameters():\n",
    "            param.requires_grad=False\n",
    "        l1_loss=torch.abs(weight_aug[re_edge_num:]).sum()\n",
    "        loss_total=(loss.sum()/mask.sum())+l1_loss*weight\n",
    "        \n",
    "        #optimizer_aug.zero_grad()\n",
    "        loss_total.backward(retain_graph=True)\n",
    "        #print(\"loss_total\",loss_total)\n",
    "        optimizer_aug.step()\n",
    "        optimizer_aug.zero_grad()\n",
    "        z_aug_detach=z_aug.detach()\n",
    "        \"\"\"\n",
    "        for name, param in aug.named_parameters():\n",
    "            param.requires_grad=False\n",
    "        print(\"z_aug1\",z_aug.requires_grad)\n",
    "        \"\"\"\n",
    "        adj_sampled_aug,adj_logits_aug = aug(z_aug_detach)\n",
    "        #adj_sampled_aug,adj_logits_aug = aug(z_ori.cuda())\n",
    "        #l=weight_aug[re_edge_num:]>0.9\n",
    "        l=(m*weight_aug1*800)>0.9\n",
    "        l_num=l.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Epoch': 0, 'train_loss': 6.032325744628906, 'loss_r': 0.3226485848426819, 'loss_l': 0.03424762934446335, 'loss_size': 1.922452449798584}\n",
      "val 0.763069677575172 test 0.734039971017993\n",
      "forget 0.6751999999999999\n",
      "weight_min tensor(0.4590, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "savebyforget\n",
      "{'Epoch': 100, 'train_loss': 5.903014659881592, 'loss_r': 0.16103118658065796, 'loss_l': 0.03415975347161293, 'loss_size': 1.9224525690078735}\n",
      "val 0.7567612607173047 test 0.7279289940828402\n",
      "forget 0.7150500000000001\n",
      "weight_min tensor(0.4589, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "{'Epoch': 200, 'train_loss': 6.069447994232178, 'loss_r': 0.3690885901451111, 'loss_l': 0.034097906202077866, 'loss_size': 1.922452449798584}\n",
      "val 0.7576138147566719 test 0.7282345127400072\n",
      "forget 0.73625\n",
      "weight_min tensor(0.4590, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "{'Epoch': 300, 'train_loss': 5.93104887008667, 'loss_r': 0.19609114527702332, 'loss_l': 0.0340939462184906, 'loss_size': 1.922452449798584}\n",
      "val 0.7580557903634827 test 0.7280563941552952\n",
      "forget 0.7288000000000001\n",
      "weight_min tensor(0.4590, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "{'Epoch': 400, 'train_loss': 6.016144752502441, 'loss_r': 0.3024804890155792, 'loss_l': 0.034014713019132614, 'loss_size': 1.922452449798584}\n",
      "val 0.759905808477237 test 0.7297935032000966\n",
      "forget 0.73315\n",
      "weight_min tensor(0.4590, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "{'Epoch': 500, 'train_loss': 5.964266300201416, 'loss_r': 0.23763591051101685, 'loss_l': 0.034002020955085754, 'loss_size': 1.922452449798584}\n",
      "val 0.7594251901944209 test 0.7282550416616351\n",
      "forget 0.6502999999999999\n",
      "weight_min tensor(0.4590, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "savebyforget\n",
      "{'Epoch': 600, 'train_loss': 6.060218811035156, 'loss_r': 0.3575904369354248, 'loss_l': 0.03394670411944389, 'loss_size': 1.922452449798584}\n",
      "val 0.7599033933099867 test 0.7277635551261925\n",
      "forget 0.7684\n",
      "weight_min tensor(0.4590, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "{'Epoch': 700, 'train_loss': 5.944576263427734, 'loss_r': 0.21302837133407593, 'loss_l': 0.033981915563344955, 'loss_size': 1.922452449798584}\n",
      "val 0.7604202391015578 test 0.7278070281366985\n",
      "forget 0.68785\n",
      "weight_min tensor(0.4590, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "{'Epoch': 800, 'train_loss': 5.8969035148620605, 'loss_r': 0.15342850983142853, 'loss_l': 0.03401636704802513, 'loss_size': 1.922452449798584}\n",
      "val 0.7579760898442216 test 0.7242953749547156\n",
      "forget 0.7308\n",
      "weight_min tensor(0.4590, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "{'Epoch': 900, 'train_loss': 5.977331161499023, 'loss_r': 0.25397181510925293, 'loss_l': 0.033980872482061386, 'loss_size': 1.922452449798584}\n",
      "val 0.7597029344282091 test 0.7268759811616954\n",
      "forget 0.6762\n",
      "weight_min tensor(0.4590, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "{'Epoch': 1000, 'train_loss': 6.07376766204834, 'loss_r': 0.374529093503952, 'loss_l': 0.033934153616428375, 'loss_size': 1.922452449798584}\n",
      "val 0.761214829126917 test 0.7285925612848689\n",
      "forget 0.7509\n",
      "weight_min tensor(0.4590, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "{'Epoch': 1100, 'train_loss': 5.989768028259277, 'loss_r': 0.2695285975933075, 'loss_l': 0.03393871337175369, 'loss_size': 1.922452449798584}\n",
      "val 0.7586330153363121 test 0.7268777925371332\n",
      "forget 0.7262000000000001\n",
      "weight_min tensor(0.4590, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "{'Epoch': 1200, 'train_loss': 6.0100202560424805, 'loss_r': 0.2948421537876129, 'loss_l': 0.033946093171834946, 'loss_size': 1.922452449798584}\n",
      "val 0.7566743146962928 test 0.72558205530733\n",
      "forget 0.7232\n",
      "weight_min tensor(0.4590, device='cuda:0', grad_fn=<MinBackward1>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[306], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3000\u001b[39m):\n\u001b[1;32m      2\u001b[0m     model_pru\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 3\u001b[0m     adj_sampled,adj_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_ori\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     weight_mask\u001b[38;5;241m=\u001b[39madj_sampled[data\u001b[38;5;241m.\u001b[39maug_pos_edge_index1[\u001b[38;5;241m0\u001b[39m],data\u001b[38;5;241m.\u001b[39maug_pos_edge_index1[\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m      5\u001b[0m     weight\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones(data\u001b[38;5;241m.\u001b[39maug_pos_edge_index1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m-\u001b[39mweight_mask\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1800\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/graph/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "Cell \u001b[0;32mIn[34], line 65\u001b[0m, in \u001b[0;36mPGNNMask.forward\u001b[0;34m(self, h, alpha, adj_orig)\u001b[0m\n\u001b[1;32m     62\u001b[0m     edge_probs \u001b[38;5;241m=\u001b[39m alpha\u001b[38;5;241m*\u001b[39medge_probs \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39malpha)\u001b[38;5;241m*\u001b[39madj_orig\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# sampling \u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m adj_sampled \u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43madj_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# making adj_sampled symmetric\u001b[39;00m\n\u001b[1;32m     67\u001b[0m adj_sampled \u001b[38;5;241m=\u001b[39m adj_sampled\u001b[38;5;241m.\u001b[39mtriu(\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[34], line 37\u001b[0m, in \u001b[0;36mPGNNMask._sample_graph\u001b[0;34m(self, sampling_weights, temperature, bias, training)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m     36\u001b[0m     bias \u001b[38;5;241m=\u001b[39m bias \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.0001\u001b[39m  \u001b[38;5;66;03m# If bias is 0, we run into problems\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     eps \u001b[38;5;241m=\u001b[39m (bias \u001b[38;5;241m-\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mbias)) \u001b[38;5;241m*\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampling_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mbias)\n\u001b[1;32m     38\u001b[0m     gate_inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(eps) \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m eps)\n\u001b[1;32m     39\u001b[0m     gate_inputs\u001b[38;5;241m=\u001b[39mgate_inputs\u001b[38;5;241m.\u001b[39mcuda()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(3000):\n",
    "    model_pru.train()\n",
    "    adj_sampled,adj_logits = mask(z_ori.cuda())\n",
    "    weight_mask=adj_sampled[data.aug_pos_edge_index1[0],data.aug_pos_edge_index1[1]]\n",
    "    weight=torch.ones(data.aug_pos_edge_index1.shape[1]).to(device)-weight_mask*600\n",
    "    z = model_pru(data.x, data.aug_pos_edge_index1,weight)\n",
    "    neg_size = data.df_aug_mask.sum()\n",
    "    neg_edge_index = negative_sampling(\n",
    "    edge_index=data.aug_pos_edge_index1,\n",
    "        num_nodes=data.num_nodes,\n",
    "        num_neg_samples=neg_size)\n",
    "    df_logits = model_pru.decode(z, data.aug_pos_edge_index1[:, data.df_aug_mask], neg_edge_index)\n",
    "    loss_size=torch.sum(weight_mask)\n",
    "    loss_r = loss_fct(df_logits[:neg_size], df_logits[neg_size:])\n",
    "    if sdf2_all_pair_without_df_mask.sum() != 0:\n",
    "        logits_sdf = (z @ z.t())[sdf2_all_pair_without_df_mask].sigmoid()\n",
    "        loss_l = loss_fct(logits_sdf, logits_ori[sdf2_all_pair_without_df_mask].sigmoid())\n",
    "    alpha = 0.5\n",
    "    beta=3\n",
    "    loss = alpha * loss_r + (1 - alpha) * loss_l+beta*loss_size\n",
    "    loss.backward(retain_graph=True)\n",
    "    # torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "    optimizer_pru.step()\n",
    "    #optimizer_pru.zero_grad()\n",
    "    optimizer_mask.step()\n",
    "    #optimizer_mask.zero_grad()\n",
    "    \n",
    "    step_log = {\n",
    "        'Epoch': epoch,\n",
    "        'train_loss': loss.item(),\n",
    "        'loss_r': loss_r.item(),\n",
    "        'loss_l': loss_l.item(),\n",
    "        'loss_size': loss_size.item(),\n",
    "    }\n",
    "    for name, param in model_ori.named_parameters():\n",
    "            param.requires_grad=False\n",
    "    if(epoch%100==0):\n",
    "        print(step_log)\n",
    "    if(epoch%100==0):\n",
    "        p=eval_val(model_pru,weight)\n",
    "        val_perf, tmp_test_perf = p\n",
    "        print(\"val\",val_perf,\"test\",tmp_test_perf)\n",
    "        auc_forget=eval_forget(model_pru,weight)\n",
    "        print(\"forget\",auc_forget)\n",
    "        #print(\"best_forget\",best_forget)\n",
    "        print(\"weight_min\",weight.min())\n",
    "        if(select==\"forget\"):\n",
    "            if(auc_forget<best_forget):\n",
    "                best_forget=auc_forget\n",
    "                ckpt = {\n",
    "                'model_state': model_pru.state_dict(),\n",
    "                'optimizer_state': optimizer_pru.state_dict(),\n",
    "            }\n",
    "                torch.save(ckpt, os.path.join(args.checkpoint_dir, 'model_delete.pt'))\n",
    "                print(\"savebyforget\")\n",
    "        else:\n",
    "            if(tmp_test_perf>best_auc):\n",
    "                tmp_test_perf>best_auc\n",
    "                best_auc=tmp_test_perf\n",
    "                ckpt = {\n",
    "                'model_state': model_pru.state_dict(),\n",
    "                'optimizer_state': optimizer_pru.state_dict(),\n",
    "            }\n",
    "                #torch.save(ckpt, os.path.join(args.checkpoint_dir, 'model_delete.pt'))\n",
    "                print(\"savebyauc\")\n",
    "    #wandb.log(step_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aaug.gcn_mean.0.weight True\n",
      "Aaug.gcn_mean.0.bias True\n",
      "Aaug.gcn_mean.1.weight True\n",
      "Aaug.gcn_mean.1.bias True\n",
      "Aaug.gcn_mean.3.weight True\n",
      "Aaug.gcn_mean.3.bias True\n"
     ]
    }
   ],
   "source": [
    "for name, param in mask.named_parameters():\n",
    "            print(name,param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6396])"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias True\n",
      "conv1.lin.weight True\n",
      "conv2.bias True\n",
      "conv2.lin.weight True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model_ori.named_parameters():\n",
    "    print(name,param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9997, device='cuda:0', grad_fn=<MinBackward1>)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "graph",
   "name": "tf2-cpu.2-11.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m123"
  },
  "kernelspec": {
   "display_name": "graph (Local)",
   "language": "python",
   "name": "graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
